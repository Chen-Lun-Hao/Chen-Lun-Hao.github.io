<!DOCTYPE html>
<html lang="zh-Hans">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="x5-fullscreen" content="true">
<meta name="full-screen" content="yes">
<meta name="theme-color" content="#317EFB" />
<meta content="width=device-width, initial-scale=1.0, maximum-scale=5.0, user-scalable=0" name="viewport">
<meta name="description" content="损失函数">
<meta property="og:type" content="article">
<meta property="og:title" content="十二大常见的损失函数">
<meta property="og:url" content="https://www.adream.icu/2021/06/12/CommonLossFunction/index.html">
<meta property="og:site_name" content="Adream blog">
<meta property="og:description" content="损失函数">
<meta property="og:locale">
<meta property="article:published_time" content="2021-06-12T15:20:59.000Z">
<meta property="article:modified_time" content="2024-03-19T13:59:28.753Z">
<meta property="article:author" content="Adream">
<meta property="article:tag" content="算法">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">

    <meta name="keywords" content="损失函数">


<title >十二大常见的损失函数</title>

<!-- Favicon -->

    <link href='/favicon.png?v=2.1.10' rel='icon' type='image/png' sizes='16x16' ></link>


    <link href='/favicon.png?v=2.1.10' rel='icon' type='image/png' sizes='32x32' ></link>


    <link href='/apple-touch-icon.png?v=2.1.10' rel='apple-touch-icon' sizes='180x180' ></link>


    <link href='/site.webmanifest' rel='manifest' ></link>


<!-- Plugin -->




    
<link rel="stylesheet" href="/css/plugins/bootstrap.row.css">

    
<link rel="stylesheet" href="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.css">

    
    
<link rel="stylesheet" href="https://npm.elemecdn.com/katex@latest/dist/katex.min.css">





<!-- Icon -->

    
<link rel="stylesheet" href="/css/plugins/font-awesome.min.css">



    
<script src="//at.alicdn.com/t/c/font_3637590_i4hyyea14ur.js"></script>



<!-- Variable -->
<script>window.ASYNC_CONFIG = {"hostname":"www.adream.icu","author":"Adream","root":"/","typed_text":["AI Developer"],"theme_version":"2.1.10","theme":{"switch":false,"default":"auto"},"favicon":{"logo":"favicon.svg","icon16":"favicon.png","icon32":"favicon.png","appleTouchIcon":"apple-touch-icon.png","webmanifest":"/site.webmanifest","visibilitychange":true,"hidden":"/failure.ico","showText":"(/≧▽≦/)咦！又好了！","hideText":"(●—●)喔哟，崩溃啦！"},"i18n":{"placeholder":"搜索文章...","empty":"找不到您查询的内容: ${query}","hits":"找到 ${hits} 条结果","hits_time":"找到 ${hits} 条结果（用时 ${time} 毫秒）","author":"本文作者：","copyright_link":"本文链接：","copyright_license_title":"版权声明：","copyright_license_content":"本博客所有文章除特别声明外，均默认采用 undefined 许可协议。","copy_success":"复制成功","copy_failure":"复制失败","open_read_mode":"进入阅读模式","exit_read_mode":"退出阅读模式","notice_outdate_message":"距离上次更新已经 undefined 天了, 文章内容可能已经过时。","sticky":"置顶","just":"刚刚","min":"分钟前","hour":"小时前","day":"天前","month":"个月前"},"swup":true,"plugin":{"flickr_justified_gallery":"https://npm.elemecdn.com/flickr-justified-gallery@latest/dist/fjGallery.min.js"},"icons":{"sun":"far fa-sun","moon":"far fa-moon","play":"icon-yunhang","email":"icon-email","next":"icon-arrow-right","calendar":"icon-rili","clock":"icon-shijian","user":"icon-yonghu","back_top":"icon-backtop","close":"icon-guanbi","search":"icon-chaxun","reward":"icon-qiandai","user_tag":"icon-yonghu1","toc_tag":"icon-liebiao","read":"icon-yuedu","arrows":"icon-arrows-h","double_arrows":"icon-angle-double-down","copy":"icon-copy"},"icontype":"symbol","highlight":{"plugin":"highlighjs","theme":true,"copy":true,"lang":true,"title":"mac","height_limit":200},"toc":{"post_title":false},"live_time":{"start_time":"2021/04/10 17:00:00","prefix":"博客已萌萌哒运行 undefined 天"},"covers":["/img/block.jpg","https://th.wallhaven.cc/orig/wq/wqvkl7.jpg","https://th.wallhaven.cc/orig/rd/rdq3v1.jpg","https://th.wallhaven.cc/orig/rd/rd183j.jpg","https://th.wallhaven.cc/orig/q2/q287wd.jpg","https://th.wallhaven.cc/orig/g7/g79ov3.jpg","https://th.wallhaven.cc/orig/q2/q2mkzr.jpg","https://th.wallhaven.cc/orig/v9/v9j3yp.jpg","https://th.wallhaven.cc/orig/y8/y8yvzx.jpg","https://th.wallhaven.cc/orig/1k/1kdw2w.jpg","https://th.wallhaven.cc/orig/57/57jqk1.jpg","https://th.wallhaven.cc/orig/57/5762p1.jpg","https://th.wallhaven.cc/orig/j3/j3zo1y.jpg","https://th.wallhaven.cc/orig/m9/m9gkky.jpg","https://th.wallhaven.cc/orig/e7/e7kv9k.jpg","https://th.wallhaven.cc/orig/ym/ym9ldk.jpg","https://th.wallhaven.cc/orig/pk/pk5259.jpg","https://th.wallhaven.cc/orig/8o/8opxpk.jpg","https://th.wallhaven.cc/orig/k7/k7yog7.jpg","https://th.wallhaven.cc/orig/v9/v93zv3.jpg","https://th.wallhaven.cc/orig/wq/wqj65q.jpg","https://th.wallhaven.cc/orig/p9/p9kg1m.jpg","https://th.wallhaven.cc/orig/8o/8o9vw1.jpg","https://th.wallhaven.cc/orig/x8/x8yy3o.jpg"],"search":{"enable":true,"type":"local","href":"https://www.google.com/search?q=site:","domain":null,"preload":true,"trigger":"auto","path":"search.xml"}};</script>
<script id="async-page-config">window.PAGE_CONFIG = {"isPost":true,"isHome":false,"postUpdate":"2024-03-19 21:59:28"};</script>

<!-- Theme mode css -->
<link data-swup-theme rel="stylesheet" href="/css/index.css?v=2.1.10" id="trm-switch-style">
<script>
    let defaultMode = ASYNC_CONFIG.theme.default !=='auto' ?  ASYNC_CONFIG.theme.default : (window.matchMedia("(prefers-color-scheme: light)").matches ? 'style-light' : 'style-dark')
    let catchMode = localStorage.getItem('theme-mode') || defaultMode;
    let type = catchMode === 'style-dark' ? 'add' : 'remove';
    document.documentElement.classList[type]('dark')
</script>

<!-- CDN -->


    
    



<!-- Site Analytics -->
 
<meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="Adream blog" type="application/atom+xml">
</head>

<body>

  <!-- app wrapper -->
  <div class="trm-app-frame">

    <!-- page preloader -->
    <div class="trm-preloader">
  <div class="loader">
    <div class="inner one"></div>
    <div class="inner two"></div>
    <div class="inner three"></div>
  </div>
  <div style="margin-top: 120px; font-weight: bold; font-size: 20px">
    努力加载中......
  </div>
</div>

    <!-- page preloader end -->

    <!-- change mode preloader -->
    <div class="trm-mode-swich-animation-frame">
    <div class="trm-mode-swich-animation">
        <i class="i-sun"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#far fa-sun"></use>
</svg></i>
        <div class="trm-horizon"></div>
        <i class="i-moon"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#far fa-moon"></use>
</svg></i>
    </div>
</div>
    <!-- change mode preloader end -->

      <!-- scroll container -->
      <div id="trm-dynamic-content" class="trm-swup-animation">
        <div id="trm-scroll-container" class="trm-scroll-container" style="opacity: 0">
            <!-- top bar -->
            <header class="trm-top-bar">
	<div class="container">
		<div class="trm-left-side">
			<!-- logo -->
<a href="/" class="trm-logo-frame trm-anima-link">
    
        <img alt="logo" src="/favicon.svg">
    
    
</a>
<!-- logo end -->
		</div>
		<div class="trm-right-side">
			<!-- menu -->
<div class="trm-menu">
    <nav>
        <ul>
            
            <li class="menu-item-has-children ">
                <a  href="/" target="">
                    首页
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/archives/" target="">
                    归档
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/projects/" target="">
                    项目
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/links/" target="">
                    友链
                </a>
                
            </li>
            
            <li class="menu-item-has-children ">
                <a  href="/about/" target="">
                    关于
                </a>
                
            </li>
            
        </ul>
    </nav>
</div>
<!-- menu end -->
			
			
		</div>
		<div class="trm-menu-btn">
			<span></span>
		</div>
	</div>
</header>
            <!-- top bar end -->

            <!-- body -->
            
<div class="trm-content-start">
    <!-- banner -->
    <div class="trm-banner">
    
    <!-- banner video cover -->
    <video autoplay="autoplay" loop muted playsinline webkit-playinginline class="trm-banner-cover">
        <source src="//cdn.moji.com/websrc/video/autumn20190924.mp4" type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'>
        Your browser does not support HTML5 video.
    </video>
    <!-- banner video cover end -->
    

    <!-- banner content -->
    <div class="trm-banner-content trm-overlay">
        <div class="container">
            <div class="row">
                
                <div class="col-lg-4"></div>
                
                <div class="col-lg-8">

                    <!-- banner title -->
                    <div class="trm-banner-text ">
                        <div class="trm-label trm-mb-20">
                            NEWS LETTER
                        </div>
                        <h1 class="trm-mb-30 trm-hsmb-font">
                            十二大常见的损失函数
                        </h1>

                        
                            <ul class="trm-breadcrumbs trm-label">
                                <li>
                                    <a href="/" class="trm-anima-link">Home</a>
                                </li>
                                <li>
                                    <span>
                                        2021
                                    </span>
                                </li>
                            </ul>
                        
                    </div>
                    <!-- banner title end -->

                    <!-- scroll hint -->
                    <span id="scroll-triger" class="trm-scroll-hint-frame">
                        <div class="trm-scroll-hint"></div>
                        <span class="trm-label">Scroll down</span>
                    </span>
                    <!-- scroll hint end -->

                </div>
            </div>
        </div>
    </div>
    <!-- banner content end -->
</div>
    <!-- banner end -->
    <div class="container">
        <div class="row">
            
                <div class="trm-page-sidebar col-lg-4 hidden-sm">
                    <!-- main card -->
                    <div class="trm-main-card-frame trm-sidebar">
    <div class="trm-main-card"> 
        <!-- card header -->
<div class="trm-mc-header">
    <div class="trm-avatar-frame trm-mb-20">
        <img alt="Avatar" class="trm-avatar trm-light-icon" src="/img/sun.jpg"> <img alt="Avatar" class="trm-avatar trm-dark-icon" src="/img/moon.jpg">
    </div>
    <h5 class="trm-name trm-mb-15">
        Adream
    </h5>
    
        <div class="trm-label">
            I`m
            <span class="trm-typed-text">
                <!-- Words for theme.user.typedText -->
            </span>
        </div>
    
</div>
<!-- card header end -->
        <!-- sidebar social -->

<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<div class="trm-social">
    
        <a href="https://github.com/chne-lun-hao" title="Github" rel="nofollow" target="_blank">
            <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-github"></use>
</svg>
        </a>
    
        <a href="https://gitee.com/lunhao2023" title="Gitee" rel="nofollow" target="_blank">
            <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-gitee"></use>
</svg>
        </a>
    
        <a href="/atom.xml" title="RSS" rel="nofollow" target="_blank">
            <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-rss"></use>
</svg>
        </a>
    
</div>

<!-- sidebar social end -->
        <!-- info -->
<div class="trm-divider trm-mb-40 trm-mt-40"></div>
<ul class="trm-table trm-mb-20">
    
        <li>
            <div class="trm-label">
                居住地:
            </div>
            <div class="trm-label trm-label-light">
                广州
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                城市:
            </div>
            <div class="trm-label trm-label-light">
                广州
            </div>
        </li>
    
        <li>
            <div class="trm-label">
                年龄:
            </div>
            <div class="trm-label trm-label-light">
                21
            </div>
        </li>
    
</ul>
<!-- info end -->

        
    <div class="trm-divider trm-mb-40 trm-mt-40"></div>
    <!-- action button -->
    <div class="text-center">
        <a href="mailto:2085127827chen@gmail.com" class="trm-btn">
            联系我
            <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-email"></use>
</svg>
        </a>
    </div>
    <!-- action button end -->

    </div>
</div>
                    <!-- main card end -->
                </div>
            
            <div class="trm-page-content col-lg-8">
                <div id="trm-content" class="trm-content">
                    <div class="trm-post-info row hidden-sm">
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <svg class="symbol-icon trm-icon" aria-hidden="true">
    <use xlink:href="#icon-rili"></use>
</svg><br>
            06/12
        </div>
    </div>
    <div class="col-sm-4">
        <div class="trm-card trm-label trm-label-light text-center">
            <svg class="symbol-icon trm-icon" aria-hidden="true">
    <use xlink:href="#icon-shijian"></use>
</svg><br>
            23:20
        </div>
    </div>
    <div class="col-sm-4">
        <div id="post-author" class="trm-card trm-label trm-label-light text-center">
            <svg class="symbol-icon trm-icon" aria-hidden="true">
    <use xlink:href="#icon-yonghu"></use>
</svg><br>
            Adream
        </div>
    </div>
</div>
<div class="trm-card ">
    <article id="article-container" class="trm-publication">
    <h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><span id="more"></span>

<p>在机器学习中，“损失函数”（Loss Function）是一个核心概念，用于量化模型的预测值与真实值之间的差异。损失函数的目的是通过最小化这个差异来训练模型，使其能够做出更准确的预测。<br>具体来说，损失函数通常定义为模型预测值与真实值之间差异的累积或平均值。在回归问题中，常用的损失函数是 <code>&lt;strong&gt;</code>均方误差（Mean Squared Error, MSE）<code>&lt;/strong&gt;</code>，它计算的是每个样本的预测值与真实值之间差异的平方的平均值。在分类问题中，常用的损失函数是 <code>&lt;strong&gt;</code>交叉熵损失（Cross-Entropy Loss）<code>&lt;/strong&gt;</code>，它衡量的是模型预测的概率分布与真实分布之间的差异。通过最小化损失函数，我们可以找到使模型在所有样本上表现最佳的参数配置。</p>
<h2 id="1-均方误差（MSE）"><a href="#1-均方误差（MSE）" class="headerlink" title="1. 均方误差（MSE）"></a>1. 均方误差（MSE）</h2><p>以下是关于均方误差损失函数（Mean Squared Error, MSE）的详细介绍，包括介绍、数学公式、工作原理、Python 代码实现以及优缺点。</p>
<h3 id="1-1-介绍"><a href="#1-1-介绍" class="headerlink" title="1.1 介绍"></a>1.1 介绍</h3><p>均方误差损失函数是回归问题中最常用的损失函数之一。它的目的是通过最小化预测值与真实值之间的平方差来训练模型，从而使模型能够更准确地预测结果。MSE 衡量的是模型预测性能的一种标准方法，常用于评估回归模型的准确性。</p>
<h3 id="1-2-数学公式"><a href="#1-2-数学公式" class="headerlink" title="1.2 数学公式"></a>1.2 数学公式</h3><p>均方误差损失函数的数学公式如下：</p>
<p>$$<br>MSE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (y_i - \hat{y}_i)^2<br>$$</p>
<p>其中：</p>
<ul>
<li>$ n $ 是样本的数量。</li>
<li>$ y_i $ 是第 $ i $ 个样本的真实值。</li>
<li>$ \hat{y}_i $ 是第 $ i $ 个样本的预测值。</li>
<li>$ \sum $ 是求和符号，表示对所有的样本进行求和。</li>
<li>$ (y_i - \hat{y}_i)^2 $ 表示第 $ i $ 个样本的真实值和预测值之间差的平方。</li>
</ul>
<h3 id="1-3-工作原理"><a href="#1-3-工作原理" class="headerlink" title="1.3 工作原理"></a>1.3 工作原理</h3><p>MSE 损失函数的工作原理是通过计算预测值与真实值之间的平方差，并将这些平方差求和后平均，来评估模型的性能。模型的训练目标是最小化这个平均平方误差值，从而使模型的预测值更接近真实值。通过最小化 MSE，模型能够更好地拟合训练数据，提高预测准确性。</p>
<h3 id="1-4-纯-Python-代码实现"><a href="#1-4-纯-Python-代码实现" class="headerlink" title="1.4 纯 Python 代码实现"></a>1.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现 MSE 损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 真实值</span></span><br><span class="line">y_true = np.array([<span class="number">3</span>, -<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">7</span>])</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">y_pred = np.array([<span class="number">2.5</span>, <span class="number">0.0</span>, <span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line"><span class="comment"># 计算MSE</span></span><br><span class="line">mse = np.mean((y_true - y_pred) ** <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MSE:&quot;</span>, mse)</span><br></pre></td></tr></table></figure>

<h3 id="1-5-优缺点"><a href="#1-5-优缺点" class="headerlink" title="1.5 优缺点"></a>1.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>数学性质良好</strong>：MSE 是一个连续可导的凸函数，确保了使用梯度下降等优化算法时能够找到全局最小值。</li>
<li><strong>对大误差的惩罚大</strong>：由于平方项的存在，较大的误差会对损失函数产生更大的影响，这有助于模型关注那些预测特别不准确的数据点。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>对异常值敏感</strong>：由于是误差的平方，异常值会对损失函数产生不成比例的影响，可能导致模型对异常值过于敏感。</li>
</ul>
<h2 id="2-平均绝对误差（MAE）"><a href="#2-平均绝对误差（MAE）" class="headerlink" title="2. 平均绝对误差（MAE）"></a>2. 平均绝对误差（MAE）</h2><h3 id="2-1-介绍"><a href="#2-1-介绍" class="headerlink" title="2.1 介绍"></a>2.1 介绍</h3><p>平均绝对误差（MAE）是回归问题中另一种常用的损失函数。它通过计算预测值与真实值之间差的绝对值的平均数来评估模型的性能。与均方误差（MSE）相比，MAE 对异常值不那么敏感，因此在数据中存在异常值时，MAE 可能是一个更好的选择。</p>
<h3 id="2-2-数学公式"><a href="#2-2-数学公式" class="headerlink" title="2.2 数学公式"></a>2.2 数学公式</h3><p>平均绝对误差的数学公式如下：</p>
<p>$$<br>MAE &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} |y_i - \hat{y}_i|<br>$$</p>
<p>其中：</p>
<ul>
<li>$ n $ 是样本的数量。</li>
<li>$ y_i $ 是第 $ i $ 个样本的真实值。</li>
<li>$ \hat{y}_i $ 是第 $ i $ 个样本的预测值。</li>
<li>$ | \cdot | $ 是绝对值符号。</li>
<li>$ \sum $ 是求和符号，表示对所有的样本进行求和。</li>
</ul>
<h3 id="2-3-工作原理"><a href="#2-3-工作原理" class="headerlink" title="2.3 工作原理"></a>2.3 工作原理</h3><p>MAE 损失函数的工作原理是通过计算预测值与真实值之间的绝对差值，并将这些绝对差值求和后平均，来评估模型的性能。模型的训练目标是最小化这个平均绝对误差值，从而使模型的预测值更接近真实值。通过最小化 MAE，模型能够更好地拟合训练数据，提高预测准确性。</p>
<h3 id="2-4-纯-Python-代码实现"><a href="#2-4-纯-Python-代码实现" class="headerlink" title="2.4 纯 Python 代码实现"></a>2.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现 MAE 损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 真实值</span></span><br><span class="line">y_true = np.array([<span class="number">3</span>, -<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">7</span>])</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">y_pred = np.array([<span class="number">2.5</span>, <span class="number">0.0</span>, <span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line"><span class="comment"># 计算MAE</span></span><br><span class="line">mae = np.mean(np.<span class="built_in">abs</span>(y_true - y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;MAE:&quot;</span>, mae)</span><br></pre></td></tr></table></figure>

<h3 id="2-5-优缺点"><a href="#2-5-优缺点" class="headerlink" title="2.5 优缺点"></a>2.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>对异常值不敏感</strong>：由于使用了绝对值，MAE 对异常值的敏感度较低，因此在数据中存在异常值时，MAE 可能是一个更好的选择。</li>
<li><strong>计算简单</strong>：MAE 的计算相对简单，只需要进行基本的算术运算。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>无法体现误差的大小</strong>：MAE 不考虑误差的绝对大小，因此在预测值与真实值相差很大时，MAE 可能无法准确反映模型的性能。</li>
</ul>
<h2 id="3-hinge-损失函数（Hinge-Loss）"><a href="#3-hinge-损失函数（Hinge-Loss）" class="headerlink" title="3. hinge 损失函数（Hinge Loss）"></a>3. hinge 损失函数（Hinge Loss）</h2><h3 id="3-1-介绍"><a href="#3-1-介绍" class="headerlink" title="3.1 介绍"></a>3.1 介绍</h3><p>铰链损失函数是一种用于分类问题的损失函数，特别是在支持向量机（Support Vector Machines, SVM）中。它的目的是通过最小化分类错误的样本的损失来训练模型，同时保持其他样本的损失为零。铰链损失函数鼓励模型正确分类支持向量（即那些位于决策边界附近的样本），同时对错误分类的样本施加较大的损失。</p>
<h3 id="3-2-数学公式"><a href="#3-2-数学公式" class="headerlink" title="3.2 数学公式"></a>3.2 数学公式</h3><p>铰链损失函数的数学公式如下：<br>对于二分类问题，公式为：</p>
<p>$$<br>L(y, f(x)) &#x3D; \max(0, 1 - y f(x))<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实标签（-1 或 1）。</li>
<li>$ f(x) $ 是第 $ i $ 个样本的预测分数。</li>
<li>$ \max(0, \cdot) $ 表示取括号内表达式的最大值，即只考虑非负部分。</li>
</ul>
<h3 id="3-3-工作原理"><a href="#3-3-工作原理" class="headerlink" title="3.3 工作原理"></a>3.3 工作原理</h3><p>铰链损失函数的工作原理是通过对错误分类的样本施加较大的损失来惩罚模型，而对正确分类的样本的损失为零。在二分类问题中，如果预测分数大于 1，则该样本被视为正类；如果预测分数小于-1，则被视为负类。如果预测分数在-1 和 1 之间，则样本被视为错误分类。铰链损失函数通过这种方式鼓励模型正确分类那些位于决策边界附近的样本，即支持向量。</p>
<h3 id="3-4-纯-Python-代码实现"><a href="#3-4-纯-Python-代码实现" class="headerlink" title="3.4 纯 Python 代码实现"></a>3.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现铰链损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hinge_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算铰链损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实标签，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测分数，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: 铰链损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测分数与真实标签的乘积</span></span><br><span class="line">    margin = y_true * y_pred</span><br><span class="line">    <span class="comment"># 只考虑非负部分</span></span><br><span class="line">    loss = np.maximum(<span class="number">0</span>, <span class="number">1</span> - margin)</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0.5</span>, -<span class="number">0.5</span>, <span class="number">1.5</span>, -<span class="number">0.5</span>])</span><br><span class="line"><span class="comment"># 计算铰链损失</span></span><br><span class="line">hinge_loss_value = hinge_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hinge Loss:&quot;</span>, hinge_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="3-5-优缺点"><a href="#3-5-优缺点" class="headerlink" title="3.5 优缺点"></a>3.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>适用于 SVM</strong>：铰链损失函数是支持向量机中的标准损失函数，适合于线性可分和近似线性可分的问题。</li>
<li><strong>对异常值不敏感</strong>：与均方误差相比，铰链损失对异常值不那么敏感。</li>
<li><strong>能够处理非线性问题</strong>：通过使用核技巧，可以扩展到非线性问题。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>对错误分类的惩罚大</strong>：铰链损失对错误分类的样本施加较大的损失，这可能导致模型在训练过程中变得过于保守。</li>
<li><strong>难以处理多分类问题</strong>：铰链损失函数主要用于二分类问题，处理多分类问题时需要使用一对多（One-vs-All）或多对多（One-vs-One）策略。</li>
<li><strong>参数敏感</strong>：铰链损失函数中的正则化参数 C 对模型的性能有很大影响，需要仔细调整。</li>
</ul>
<h2 id="4-指数损失函数（Exponential-Loss）"><a href="#4-指数损失函数（Exponential-Loss）" class="headerlink" title="4. 指数损失函数（Exponential Loss）"></a>4. 指数损失函数（Exponential Loss）</h2><h3 id="4-1-介绍"><a href="#4-1-介绍" class="headerlink" title="4.1 介绍"></a>4.1 介绍</h3><p>指数损失函数，也称为对数损失函数（Log Loss）的一种形式，是一种常用于二分类问题的损失函数。它度量的是模型预测的概率与真实标签之间的差异。指数损失函数鼓励模型对正类样本的预测概率接近 1，对负类样本的预测概率接近 0。</p>
<h3 id="4-2-数学公式"><a href="#4-2-数学公式" class="headerlink" title="4.2 数学公式"></a>4.2 数学公式</h3><p>指数损失函数的数学公式如下：<br>对于二分类问题，公式为：</p>
<p>$$<br>L(y, p) &#x3D; -y \log(p) - (1 - y) \log(1 - p)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实标签（0 或 1）。</li>
<li>$ p $ 是模型对正类的预测概率。</li>
<li>$ \log $ 是自然对数。</li>
</ul>
<h3 id="4-3-工作原理"><a href="#4-3-工作原理" class="headerlink" title="4.3 工作原理"></a>4.3 工作原理</h3><p>指数损失函数的工作原理是通过对正类样本的预测概率进行惩罚，如果预测概率小于真实标签，而对负类样本的预测概率进行奖励，如果预测概率大于真实标签。这种惩罚和奖励机制使得模型在训练过程中逐渐调整参数，以便对正类样本的预测概率接近 1，对负类样本的预测概率接近 0。</p>
<h3 id="4-4-纯-Python-代码实现"><a href="#4-4-纯-Python-代码实现" class="headerlink" title="4.4 纯 Python 代码实现"></a>4.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现指数损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">exponential_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算指数损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实标签，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测概率，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: 指数损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算指数损失</span></span><br><span class="line">    loss = -y_true * np.log(y_pred) - (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred)</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.8</span>])</span><br><span class="line"><span class="comment"># 计算指数损失</span></span><br><span class="line">exponential_loss_value = exponential_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Exponential Loss:&quot;</span>, exponential_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="4-5-优缺点"><a href="#4-5-优缺点" class="headerlink" title="4.5 优缺点"></a>4.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>适用于二分类问题</strong>：指数损失函数适合于二分类问题，可以有效地度量模型对正类和负类样本的预测概率。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>对预测概率的敏感度</strong>：指数损失函数对预测概率的微小变化非常敏感，可能导致模型在训练过程中对预测概率的调整不够平滑。</li>
<li><strong>可能需要正则化</strong>：在实际应用中，指数损失函数可能需要通过添加正则化项来防止过拟合。<br>通过以上介绍，您可以对指数损失函数有一个更详细的了解。</li>
</ul>
<h2 id="5-huber-损失函数（Huber-Loss）"><a href="#5-huber-损失函数（Huber-Loss）" class="headerlink" title="5. huber 损失函数（Huber Loss）"></a>5. huber 损失函数（Huber Loss）</h2><h3 id="5-1-介绍"><a href="#5-1-介绍" class="headerlink" title="5.1 介绍"></a>5.1 介绍</h3><p>Huber 损失函数是一种在回归问题中常用的损失函数，它结合了均方误差（MSE）和绝对损失（MAE）的特点。当误差较小时，Huber 损失函数接近于 MSE，这样可以保证损失函数的连续可导性；当误差较大时，Huber 损失函数变为绝对损失，这样可以减少大误差对损失函数的影响。Huber 损失函数适用于包含异常值的数据集。</p>
<h3 id="5-2-数学公式"><a href="#5-2-数学公式" class="headerlink" title="5.2 数学公式"></a>5.2 数学公式</h3><p>Huber 损失函数的数学公式如下：</p>
<p>$$<br>L(a) &#x3D; \begin{cases}<br>    \frac{1}{2}a^2 &amp; \text{for } |a| \leq \delta \<br>    \delta(|a| - \frac{1}{2}\delta) &amp; \text{for } |a| &gt; \delta<br>\end{cases}<br>$$</p>
<p>其中：</p>
<ul>
<li>$ a $ 是预测值与真实值之间的差值。</li>
<li>$ \delta $ 是 Huber 损失函数的参数，称为“delta”。</li>
</ul>
<h3 id="5-3-工作原理"><a href="#5-3-工作原理" class="headerlink" title="5.3 工作原理"></a>5.3 工作原理</h3><p>Huber 损失函数的工作原理是通过对预测值与真实值之间的差值进行平方，当差值的绝对值小于或等于 delta 时；当差值的绝对值大于 delta 时，使用 delta 乘以差值的绝对值减去 delta 的一半。这种方法使得 Huber 损失函数在预测值与真实值之间的差值较小时，接近于均方误差，而在差值较大时，接近于绝对损失。</p>
<h3 id="5-4-纯-Python-代码实现"><a href="#5-4-纯-Python-代码实现" class="headerlink" title="5.4 纯 Python 代码实现"></a>5.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现 Huber 损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">huber_loss</span>(<span class="params">y_true, y_pred, delta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算Huber损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实值，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测值，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param delta: Huber损失函数的参数，即delta。</span></span><br><span class="line"><span class="string">    :return: Huber损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测值与真实值之间的差值</span></span><br><span class="line">    diff = y_true - y_pred</span><br><span class="line">    <span class="comment"># 计算差值的绝对值</span></span><br><span class="line">    diff_abs = np.<span class="built_in">abs</span>(diff)</span><br><span class="line">    <span class="comment"># 判断差值的绝对值是否大于delta</span></span><br><span class="line">    condition = diff_abs &lt;= delta</span><br><span class="line">    <span class="comment"># 当差值的绝对值小于等于delta时，使用均方误差</span></span><br><span class="line">    mse = <span class="number">0.5</span> * np.square(diff)</span><br><span class="line">    <span class="comment"># 当差值的绝对值大于delta时，使用绝对损失</span></span><br><span class="line">    mae = delta * (diff_abs - <span class="number">0.5</span> * delta)</span><br><span class="line">    <span class="comment"># 合并两种情况</span></span><br><span class="line">    loss = np.where(condition, mse, mae)</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">3</span>, -<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">7</span>])</span><br><span class="line">y_pred = np.array([<span class="number">2.5</span>, <span class="number">0.0</span>, <span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line">delta = <span class="number">1.0</span></span><br><span class="line"><span class="comment"># 计算Huber损失</span></span><br><span class="line">huber_loss_value = huber_loss(y_true, y_pred, delta)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Huber Loss:&quot;</span>, huber_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="5-5-优缺点"><a href="#5-5-优缺点" class="headerlink" title="5.5 优缺点"></a>5.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>对异常值不敏感</strong>：Huber 损失函数对异常值不敏感，适用于包含异常值的数据集。</li>
<li><strong>平滑过渡</strong>：Huber 损失函数在均方误差和绝对损失之间平滑过渡，可以减少模型对异常值的敏感度。</li>
<li><strong>易于实现</strong>：Huber 损失函数的实现相对简单，可以通过调整 delta 参数来适应不同的数据集。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>参数敏感</strong>：Huber 损失函数的性能依赖于 delta 参数的选择，选择不当可能会影响模型的性能。</li>
<li><strong>计算复杂度</strong>：与均方误差相比，Huber 损失函数的计算复杂度略高，因为需要对每个样本的差值进行判断和计算。</li>
</ul>
<h2 id="6-KL-散度函数（相对熵）"><a href="#6-KL-散度函数（相对熵）" class="headerlink" title="6. KL 散度函数（相对熵）"></a>6. KL 散度函数（相对熵）</h2><h3 id="6-1-介绍"><a href="#6-1-介绍" class="headerlink" title="6.1 介绍"></a>6.1 介绍</h3><p>KL 散度函数，也称为相对熵，是一种衡量两个概率分布之间差异的度量。在机器学习中，KL 散度常用于比较模型的预测分布与真实分布或另一个模型的分布。KL 散度是非对称的，表示从分布 $ P $ 到分布 $ Q $ 的差异，而 $ Q $ 到 $ P $ 的差异可能不同。</p>
<h3 id="6-2-数学公式"><a href="#6-2-数学公式" class="headerlink" title="6.2 数学公式"></a>6.2 数学公式</h3><p>KL 散度函数的数学公式如下：</p>
<p>$$<br>D_{KL}(P || Q) &#x3D; \sum_{i} P(i) \log_2 \left( \frac{P(i)}{Q(i)} \right)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ P $ 和 $ Q $ 是两个概率分布。</li>
<li>$ P(i) $ 是分布 $ P $ 中第 $ i $ 个事件的概率。</li>
<li>$ Q(i) $ 是分布 $ Q $ 中第 $ i $ 个事件的概率。</li>
<li>$ \log_2 $ 是以 2 为底的对数。</li>
</ul>
<h3 id="6-3-工作原理"><a href="#6-3-工作原理" class="headerlink" title="6.3 工作原理"></a>6.3 工作原理</h3><p>KL 散度的工作原理是通过比较两个概率分布的每个事件的对数比值，来度量它们之间的差异。如果两个分布完全相同，那么 KL 散度为 0。如果一个分布的概率大于另一个分布的概率，那么这个分布的概率的对数比值会大于 1，从而增加 KL 散度的值。KL 散度越大，表示两个分布的差异越大。</p>
<h3 id="6-4-纯-Python-代码实现"><a href="#6-4-纯-Python-代码实现" class="headerlink" title="6.4 纯 Python 代码实现"></a>6.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现 KL 散度函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kl_divergence</span>(<span class="params">P, Q</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算KL散度函数的值。</span></span><br><span class="line"><span class="string">    :param P: 第一个概率分布，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param Q: 第二个概率分布，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: KL散度函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算KL散度</span></span><br><span class="line">    kl = np.<span class="built_in">sum</span>(P * np.log2(P / Q))</span><br><span class="line">    <span class="keyword">return</span> kl</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">P = np.array([<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.4</span>])</span><br><span class="line">Q = np.array([<span class="number">0.2</span>, <span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.3</span>])</span><br><span class="line"><span class="comment"># 计算KL散度</span></span><br><span class="line">kl_divergence_value = kl_divergence(P, Q)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;KL Divergence:&quot;</span>, kl_divergence_value)</span><br></pre></td></tr></table></figure>

<h3 id="6-5-优缺点"><a href="#6-5-优缺点" class="headerlink" title="6.5 优缺点"></a>6.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>信息论背景</strong>：KL 散度是基于信息论的概念，具有坚实的理论基础。</li>
<li><strong>非对称性</strong>：KL 散度反映了从分布 $ P $ 到分布 $ Q $ 的转换信息，这有助于理解数据或模型在转换过程中的变化。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>不满足对称性</strong>：KL 散度不满足对称性，即 $ D*{KL}(P || Q) \neq D*{KL}(Q || P) $，这使得它在对称情况下可能不是衡量两个分布差异的最佳选择。</li>
<li><strong>对极值敏感</strong>：KL 散度对概率分布的极值非常敏感，这可能导致在处理具有不同峰值和形状的分布时产生较大的差异。</li>
<li><strong>不适用于非概率分布</strong>：KL 散度是专门为概率分布设计的，不适用于非概率分布的比较。</li>
</ul>
<h2 id="7-交叉熵损失（Cross-Entropy-Loss）"><a href="#7-交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="7. 交叉熵损失（Cross-Entropy Loss）"></a>7. 交叉熵损失（Cross-Entropy Loss）</h2><h3 id="7-1-介绍"><a href="#7-1-介绍" class="headerlink" title="7.1 介绍"></a>7.1 介绍</h3><p>交叉熵损失是一种常用于分类问题的损失函数，特别是在神经网络中。它的目的是度量实际输出与期望输出之间的差异。交叉熵损失鼓励模型输出概率分布与真实标签的分布尽可能接近。</p>
<h3 id="7-2-数学公式"><a href="#7-2-数学公式" class="headerlink" title="7.2 数学公式"></a>7.2 数学公式</h3><p>对于二分类问题，交叉熵损失的数学公式如下：</p>
<p>$$<br>L(y, p) &#x3D; -y \log(p)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实标签（0 或 1）。</li>
<li>$ p $ 是模型对正类的预测概率。</li>
<li>$ \log $ 是自然对数。<br>对于多分类问题，交叉熵损失的数学公式通常采用 softmax 函数将输出转换为概率分布，然后计算对数似然损失：<br>$$<br>L(y, p) &#x3D; -\sum_{i} y_i \log(p_i)<br>$$<br>其中：</li>
<li>$ y $ 是真实标签的 one-hot 编码。</li>
<li>$ p $ 是模型的预测概率分布。</li>
<li>$ \log $ 是自然对数。</li>
</ul>
<h3 id="7-3-工作原理"><a href="#7-3-工作原理" class="headerlink" title="7.3 工作原理"></a>7.3 工作原理</h3><p>交叉熵损失的工作原理是通过计算模型预测的概率分布与真实标签的分布之间的对数似然比，来度量它们之间的差异。对于二分类问题，如果模型正确预测了正类，那么对数似然比为正，交叉熵损失为负；如果模型错误预测了正类，那么对数似然比为负，交叉熵损失为正。对于多分类问题，交叉熵损失通过计算每个类别的对数似然比，然后求和来评估整个概率分布的差异。</p>
<h3 id="7-4-纯-Python-代码实现"><a href="#7-4-纯-Python-代码实现" class="headerlink" title="7.4 纯 Python 代码实现"></a>7.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现交叉熵损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算交叉熵损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实标签，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测概率，二维数组或矩阵。</span></span><br><span class="line"><span class="string">    :return: 交叉熵损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算交叉熵损失</span></span><br><span class="line">    loss = -np.<span class="built_in">sum</span>(y_true * np.log(y_pred))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = np.array([[<span class="number">0.9</span>, <span class="number">0.1</span>],</span><br><span class="line">                   [<span class="number">0.1</span>, <span class="number">0.9</span>],</span><br><span class="line">                   [<span class="number">0.8</span>, <span class="number">0.2</span>],</span><br><span class="line">                   [<span class="number">0.2</span>, <span class="number">0.8</span>]])</span><br><span class="line"><span class="comment"># 计算交叉熵损失</span></span><br><span class="line">cross_entropy_loss_value = cross_entropy_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cross-Entropy Loss:&quot;</span>, cross_entropy_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="7-5-优缺点"><a href="#7-5-优缺点" class="headerlink" title="7.5 优缺点"></a>7.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>适用于分类问题</strong>：交叉熵损失适合于分类问题，可以有效地度量模型对不同类别的预测概率。</li>
<li><strong>连续可导</strong>：交叉熵损失是连续可导的，这使得可以使用梯度下降等优化算法来寻找最小化损失函数的参数。</li>
<li><strong>对异常值不敏感</strong>：交叉熵损失对异常值不敏感，适用于各种类型的数据集。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>对预测概率的敏感度</strong>：交叉熵损失对预测概率的微小变化非常敏感，可能导致模型在训练过程中对预测概率的调整不够平滑。</li>
<li><strong>计算复杂度</strong>：交叉熵损失的计算复杂度较高，特别是在多分类问题中，需要对每个类别进行计算。</li>
</ul>
<h2 id="8-逻辑回归损失函数（Logistic-Loss）"><a href="#8-逻辑回归损失函数（Logistic-Loss）" class="headerlink" title="8. 逻辑回归损失函数（Logistic Loss）"></a>8. 逻辑回归损失函数（Logistic Loss）</h2><h3 id="8-1-介绍"><a href="#8-1-介绍" class="headerlink" title="8.1 介绍"></a>8.1 介绍</h3><p>逻辑回归损失函数是一种用于二分类问题的损失函数。它度量的是模型预测的概率与真实标签之间的差异。逻辑回归损失函数鼓励模型对正类样本的预测概率接近 1，对负类样本的预测概率接近 0。</p>
<h3 id="8-2-数学公式"><a href="#8-2-数学公式" class="headerlink" title="8.2 数学公式"></a>8.2 数学公式</h3><p>逻辑回归损失函数的数学公式如下：</p>
<p>$$<br>L(y, p) &#x3D; -y \log(p) - (1 - y) \log(1 - p)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实标签（0 或 1）。</li>
<li>$ p $ 是模型对正类的预测概率。</li>
<li>$ \log $ 是自然对数。</li>
</ul>
<h3 id="8-3-工作原理"><a href="#8-3-工作原理" class="headerlink" title="8.3 工作原理"></a>8.3 工作原理</h3><p>逻辑回归损失函数的工作原理是通过对正类样本的预测概率进行惩罚，如果预测概率小于真实标签，而对负类样本的预测概率进行奖励，如果预测概率大于真实标签。这种惩罚和奖励机制使得模型在训练过程中逐渐调整参数，以便对正类样本的预测概率接近 1，对负类样本的预测概率接近 0。</p>
<h3 id="8-4-纯-Python-代码实现"><a href="#8-4-纯-Python-代码实现" class="headerlink" title="8.4 纯 Python 代码实现"></a>8.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现逻辑回归损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算逻辑回归损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实标签，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测概率，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: 逻辑回归损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算逻辑回归损失</span></span><br><span class="line">    loss = -y_true * np.log(y_pred) - (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> - y_pred)</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.8</span>])</span><br><span class="line"><span class="comment"># 计算逻辑回归损失</span></span><br><span class="line">logistic_loss_value = logistic_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Logistic Loss:&quot;</span>, logistic_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="8-5-优缺点"><a href="#8-5-优缺点" class="headerlink" title="8.5 优缺点"></a>8.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>适用于二分类问题</strong>：逻辑回归损失函数适合于二分类问题，可以有效地度量模型对正类和负类样本的预测概率。</li>
<li><strong>易于理解和实现</strong>：逻辑回归损失函数的公式简单易懂，易于在编程语言中实现。</li>
<li><strong>对异常值不敏感</strong>：逻辑回归损失函数对异常值不敏感，适用于各种类型的数据集。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>对预测概率的敏感度</strong>：逻辑回归损失函数对预测概率的微小变化非常敏感，可能导致模型在训练过程中对预测概率的调整不够平滑。</li>
<li><strong>可能需要正则化</strong>：在实际应用中，逻辑回归损失函数可能需要通过添加正则化项来防止过拟合。</li>
</ul>
<h2 id="9-对数双曲余弦损失（Log-Cosh-Loss）"><a href="#9-对数双曲余弦损失（Log-Cosh-Loss）" class="headerlink" title="9. 对数双曲余弦损失（Log-Cosh Loss）"></a>9. 对数双曲余弦损失（Log-Cosh Loss）</h2><h3 id="9-1-介绍"><a href="#9-1-介绍" class="headerlink" title="9.1 介绍"></a>9.1 介绍</h3><p>对数双曲余弦损失是一种用于回归问题的损失函数，它是对均方误差（MSE）的一种平滑化。它在预测值与真实值之间差值较大时，惩罚相对较轻，而在差值较小时，惩罚相对较重。这种性质使得 Log-Cosh Loss 在处理包含异常值的数据时更加鲁棒。</p>
<h3 id="9-2-数学公式"><a href="#9-2-数学公式" class="headerlink" title="9.2 数学公式"></a>9.2 数学公式</h3><p>对数双曲余弦损失的数学公式如下：</p>
<p>$$<br>L(y, \hat{y}) &#x3D; \log\left(cosh(y - \hat{y})\right)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实值。</li>
<li>$ \hat{y} $ 是第 $ i $ 个样本的预测值。</li>
<li>$ cosh $ 是双曲余弦函数。</li>
<li>$ \log $ 是自然对数。</li>
</ul>
<h3 id="9-3-工作原理"><a href="#9-3-工作原理" class="headerlink" title="9.3 工作原理"></a>9.3 工作原理</h3><p>对数双曲余弦损失的工作原理是通过计算预测值与真实值之间的双曲余弦值，然后取其对数。当预测值与真实值之间的差值较大时，双曲余弦值接近于 1，对数值接近于 0，因此损失较小；当预测值与真实值之间的差值较小时，双曲余弦值接近于 0，对数值较大，因此损失较大。这种性质使得 Log-Cosh Loss 在处理包含异常值的数据时更加鲁棒。</p>
<h3 id="9-4-纯-Python-代码实现"><a href="#9-4-纯-Python-代码实现" class="headerlink" title="9.4 纯 Python 代码实现"></a>9.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现对数双曲余弦损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_cosh_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算对数双曲余弦损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实值，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测值，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: 对数双曲余弦损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算预测值与真实值之间的差值</span></span><br><span class="line">    diff = y_true - y_pred</span><br><span class="line">    <span class="comment"># 计算双曲余弦值</span></span><br><span class="line">    cosh_diff = np.cosh(diff)</span><br><span class="line">    <span class="comment"># 计算对数双曲余弦损失</span></span><br><span class="line">    loss = np.log(cosh_diff)</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">3</span>, -<span class="number">0.5</span>, <span class="number">2</span>, <span class="number">7</span>])</span><br><span class="line">y_pred = np.array([<span class="number">2.5</span>, <span class="number">0.0</span>, <span class="number">2</span>, <span class="number">8</span>])</span><br><span class="line"><span class="comment"># 计算对数双曲余弦损失</span></span><br><span class="line">log_cosh_loss_value = log_cosh_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Log-Cosh Loss:&quot;</span>, log_cosh_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="9-5-优缺点"><a href="#9-5-优缺点" class="headerlink" title="9.5 优缺点"></a>9.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>对异常值不敏感</strong>：对数双曲余弦损失对异常值不敏感，适用于包含异常值的数据集。</li>
<li><strong>平滑过渡</strong>：对数双曲余弦损失在均方误差和绝对损失之间平滑过渡，可以减少模型对异常值的敏感度。</li>
<li><strong>易于实现</strong>：对数双曲余弦损失的实现相对简单，可以通过调整双曲余弦函数的参数来适应不同的数据集。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>计算复杂度</strong>：与均方误差相比，对数双曲余弦损失的计算复杂度略高，因为需要对每个样本的差值进行双曲余弦计算和对数计算。</li>
</ul>
<h2 id="10-对数双曲正切损失（Log-Hyperbolic-Tangent-Loss）"><a href="#10-对数双曲正切损失（Log-Hyperbolic-Tangent-Loss）" class="headerlink" title="10. 对数双曲正切损失（Log-Hyperbolic Tangent Loss）"></a>10. 对数双曲正切损失（Log-Hyperbolic Tangent Loss）</h2><h3 id="10-1-介绍"><a href="#10-1-介绍" class="headerlink" title="10.1 介绍"></a>10.1 介绍</h3><p>对数双曲正切损失函数是一种结合了对数损失和双曲正切函数的损失函数。它鼓励模型输出概率分布与真实标签的分布尽可能接近，并且在模型预测概率接近 1 或-1 时更加稳定。</p>
<h3 id="10-2-数学公式"><a href="#10-2-数学公式" class="headerlink" title="10.2 数学公式"></a>10.2 数学公式</h3><p>对数双曲正切损失的数学公式如下：</p>
<p>$$<br>L(y, f(x)) &#x3D; -y \log(1 + \exp(f(x))) - (1 - y) \log(1 + \exp(-f(x)))<br>$$</p>
<p>其中：</p>
<ul>
<li>$ y $ 是第 $ i $ 个样本的真实标签（0 或 1）。</li>
<li>$ f(x) $ 是模型对正类的预测分数。</li>
<li>$ \log $ 是自然对数。</li>
<li>$ \exp $ 是指数函数。</li>
</ul>
<h3 id="10-3-工作原理"><a href="#10-3-工作原理" class="headerlink" title="10.3 工作原理"></a>10.3 工作原理</h3><p>对数双曲正切损失的工作原理是通过对数函数和双曲正切函数的组合。当模型预测概率接近 1 时，对数函数接近其上限，此时损失主要受对数函数的影响；当模型预测概率接近-1 时，对数函数接近其下限，此时损失主要受双曲正切函数的影响。这种设计使得对数双曲正切损失在模型预测概率接近 1 或-1 时更加稳定。</p>
<h3 id="10-4-纯-Python-代码实现"><a href="#10-4-纯-Python-代码实现" class="headerlink" title="10.4 纯 Python 代码实现"></a>10.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现对数双曲正切损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_hyperbolic_tangent_loss</span>(<span class="params">y_true, y_pred</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算对数双曲正切损失函数的值。</span></span><br><span class="line"><span class="string">    :param y_true: 真实标签，一维数组或向量。</span></span><br><span class="line"><span class="string">    :param y_pred: 预测概率，一维数组或向量。</span></span><br><span class="line"><span class="string">    :return: 对数双曲正切损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算对数双曲正切损失</span></span><br><span class="line">    loss = -y_true * np.log(<span class="number">1</span> + np.exp(y_pred)) - (<span class="number">1</span> - y_true) * np.log(<span class="number">1</span> + np.exp(-y_pred))</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">y_true = np.array([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">y_pred = np.array([<span class="number">0.9</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.8</span>])</span><br><span class="line"><span class="comment"># 计算对数双曲正切损失</span></span><br><span class="line">log_hyperbolic_tangent_loss_value = log_hyperbolic_tangent_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Log-Hyperbolic Tangent Loss:&quot;</span>, log_hyperbolic_tangent_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="10-5-优缺点"><a href="#10-5-优缺点" class="headerlink" title="10.5 优缺点"></a>10.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>稳定性</strong>：对数双曲正切损失在预测概率接近 1 或-1 时更加稳定，适用于输出概率非常接近 1 或-1 的情况。</li>
<li><strong>易于实现</strong>：对数双曲正切损失的实现相对简单，可以通过调整双曲正切函数的参数来适应不同的数据集。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>计算复杂度</strong>：与均方误差或交叉熵损失相比，对数双曲正切损失的计算复杂度较高，因为需要对每个样本的预测概率进行双曲正切计算和对数计算。</li>
</ul>
<h2 id="11-余弦相似度损失（Cosine-Similarity-Loss）"><a href="#11-余弦相似度损失（Cosine-Similarity-Loss）" class="headerlink" title="11. 余弦相似度损失（Cosine Similarity Loss）"></a>11. 余弦相似度损失（Cosine Similarity Loss）</h2><h3 id="11-1-介绍"><a href="#11-1-介绍" class="headerlink" title="11.1 介绍"></a>11.1 介绍</h3><p>余弦相似度损失是一种用于衡量两个向量之间相似度的损失函数。在机器学习中，特别是在表示学习或嵌入空间中，余弦相似度损失常用于度量两个向量表示的相似度。它鼓励模型学习到能够保持数据点之间相似度的特征表示。</p>
<h3 id="11-2-数学公式"><a href="#11-2-数学公式" class="headerlink" title="11.2 数学公式"></a>11.2 数学公式</h3><p>余弦相似度损失的数学公式如下：</p>
<p>$$<br>L(x, \hat{x}) &#x3D; 1 - \cos(\theta)<br>$$</p>
<p>其中：</p>
<ul>
<li>$ x $ 是第 $ i $ 个样本的真实向量。</li>
<li>$ \hat{x} $ 是第 $ i $ 个样本的预测向量。</li>
<li>$ \theta $ 是向量 $ x $ 和 $ \hat{x} $ 之间的夹角。</li>
<li>$ \cos(\theta) $ 是向量 $ x $ 和 $ \hat{x} $ 之间的余弦相似度。</li>
</ul>
<h3 id="11-3-工作原理"><a href="#11-3-工作原理" class="headerlink" title="11.3 工作原理"></a>11.3 工作原理</h3><p>余弦相似度损失的工作原理是通过计算两个向量之间的余弦相似度，然后取其相反数。当两个向量表示相同或相似的实体时，它们的夹角接近 0 度，余弦相似度接近 1，此时损失接近 0；当两个向量表示不同的实体时，它们的夹角接近 90 度，余弦相似度接近 0，此时损失接近 1。这种性质使得余弦相似度损失能够有效地度量两个向量之间的相似度。</p>
<h3 id="11-4-纯-Python-代码实现"><a href="#11-4-纯-Python-代码实现" class="headerlink" title="11.4 纯 Python 代码实现"></a>11.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 NumPy 库来实现余弦相似度损失函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity_loss</span>(<span class="params">x, x_hat</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算余弦相似度损失函数的值。</span></span><br><span class="line"><span class="string">    :param x: 真实向量，二维数组或矩阵。</span></span><br><span class="line"><span class="string">    :param x_hat: 预测向量，二维数组或矩阵。</span></span><br><span class="line"><span class="string">    :return: 余弦相似度损失函数的值。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 计算余弦相似度</span></span><br><span class="line">    cos_sim = np.dot(x, x_hat) / (np.linalg.norm(x) * np.linalg.norm(x_hat))</span><br><span class="line">    <span class="comment"># 计算余弦相似度损失</span></span><br><span class="line">    loss = <span class="number">1</span> - cos_sim</span><br><span class="line">    <span class="comment"># 计算平均损失</span></span><br><span class="line">    <span class="keyword">return</span> np.mean(loss)</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">0</span>, -<span class="number">1</span>],</span><br><span class="line">              [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">x_hat = np.array([[<span class="number">0.5</span>, -<span class="number">0.5</span>, <span class="number">0.5</span>],</span><br><span class="line">                  [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]])</span><br><span class="line"><span class="comment"># 计算余弦相似度损失</span></span><br><span class="line">cosine_similarity_loss_value = cosine_similarity_loss(x, x_hat)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Cosine Similarity Loss:&quot;</span>, cosine_similarity_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="11-5-优缺点"><a href="#11-5-优缺点" class="headerlink" title="11.5 优缺点"></a>11.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>度量相似度</strong>：余弦相似度损失能够直接度量两个向量之间的相似度，无需额外的参数。</li>
<li><strong>易于理解</strong>：余弦相似度损失的公式简单易懂，易于在编程语言中实现。</li>
<li><strong>适用于嵌入空间</strong>：在表示学习或嵌入空间中，余弦相似度损失能够有效地度量数据点之间的相似度。<br><strong>缺点：</strong></li>
<li><strong>对方向不敏感</strong>：余弦相似度损失仅考虑向量的方向，不考虑向量的幅度，这可能导致在某些情况下对结果不够敏感。</li>
<li><strong>不适用于非线性关系</strong>：余弦相似度损失基于余弦函数，仅适用于度量向量之间的线性关系，对于非线性关系可能不够适用。</li>
</ul>
<h2 id="12-感知损失（Perceptual-Loss）"><a href="#12-感知损失（Perceptual-Loss）" class="headerlink" title="12. 感知损失（Perceptual Loss）"></a>12. 感知损失（Perceptual Loss）</h2><p>感知损失（Perceptual Loss）是一种在图像处理和计算机视觉中使用的损失函数，它利用预训练的卷积神经网络（CNN）来评估两个图像之间的感知差异。这种损失函数的目的是使生成模型产生的图像在人类视觉上与真实图像难以区分。感知损失通常用于图像到图像的翻译任务，如风格迁移、超分辨率、去噪等。</p>
<h3 id="12-1-介绍"><a href="#12-1-介绍" class="headerlink" title="12.1 介绍"></a>12.1 介绍</h3><p>感知损失是基于人类视觉感知的特点，它不是简单地计算图像像素级的差异，而是考虑图像的整体结构和内容。感知损失通常使用预训练的 CNN 模型，如 VGG19，来提取图像的高级特征，然后基于这些高级特征计算损失。</p>
<h3 id="12-2-数学公式"><a href="#12-2-数学公式" class="headerlink" title="12.2 数学公式"></a>12.2 数学公式</h3><p>感知损失的数学公式通常如下：</p>
<p>$$<br>L_{perceptual}(\text{G}(x), x) &#x3D; \frac{1}{H \times W} \sum_{i, j} \left| \text{VGG19}(G(x))<em>{i, j} - \text{VGG19}(x)</em>{i, j} \right|<br>$$</p>
<p>其中：</p>
<ul>
<li>$ G(x) $ 是生成模型生成的图像。</li>
<li>$ x $ 是真实图像。</li>
<li>$ H \times W $ 是图像的高度和宽度。</li>
<li>$ \text{VGG19} $ 是预训练的 VGG19 CNN 模型。</li>
<li>$ \left| \cdot \right| $ 是绝对值。</li>
</ul>
<h3 id="12-3-工作原理"><a href="#12-3-工作原理" class="headerlink" title="12.3 工作原理"></a>12.3 工作原理</h3><p>感知损失的工作原理是使用预训练的 CNN 模型提取图像的高级特征，然后比较生成图像和真实图像在这些高级特征上的差异。这种方法考虑了图像的结构和内容，而不仅仅是像素级的差异。通过最小化感知损失，生成模型学习到的特征更加接近真实图像的特征，从而在人类视觉上难以区分。</p>
<h3 id="12-4-纯-Python-代码实现"><a href="#12-4-纯-Python-代码实现" class="headerlink" title="12.4 纯 Python 代码实现"></a>12.4 纯 Python 代码实现</h3><p>在 Python 中，可以使用 PyTorch 库来实现感知损失函数。以下是一个简单的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.models <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 加载预训练的VGG19模型</span></span><br><span class="line">vgg19 = models.vgg19(pretrained=<span class="literal">True</span>).<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 定义感知损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">perceptual_loss</span>(<span class="params">G, x</span>):</span><br><span class="line">    <span class="comment"># 提取VGG19模型的特征层</span></span><br><span class="line">    G_features = vgg19(F.interpolate(G, size=(<span class="number">224</span>, <span class="number">224</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>))</span><br><span class="line">    x_features = vgg19(F.interpolate(x, size=(<span class="number">224</span>, <span class="number">224</span>), mode=<span class="string">&#x27;bilinear&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = F.mse_loss(G_features, x_features)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">G = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)  <span class="comment"># 生成图像</span></span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">256</span>, <span class="number">256</span>)  <span class="comment"># 真实图像</span></span><br><span class="line"><span class="comment"># 计算感知损失</span></span><br><span class="line">perceptual_loss_value = perceptual_loss(G, x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Perceptual Loss:&quot;</span>, perceptual_loss_value)</span><br></pre></td></tr></table></figure>

<h3 id="12-5-优缺点"><a href="#12-5-优缺点" class="headerlink" title="12.5 优缺点"></a>12.5 优缺点</h3><p><strong>优点：</strong></p>
<ul>
<li><strong>考虑图像结构</strong>：感知损失考虑了图像的高级特征，能够捕捉图像的结构和内容。</li>
<li><strong>适用于图像翻译任务</strong>：在图像到图像的翻译任务中，感知损失能够有效地评估生成图像的质量。</li>
<li><strong>易于实现</strong>：感知损失可以通过使用预训练的 CNN 模型来实现，无需复杂的计算。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>依赖预训练模型</strong>：感知损失依赖于预训练的 CNN 模型，这可能需要大量的计算资源。</li>
<li><strong>难以解释</strong>：由于感知损失基于复杂的高级特征，其计算过程难以解释和理解。</li>
<li><strong>对模型敏感</strong>：感知损失的性能可能受到所选 CNN 模型的影响，不同的模型可能产生不同的结果。</li>
</ul>

</article>
    
    

<ul class="trm-post-copyright">
    <li class="trm-post-copyright-author">
        <strong>本文作者：</strong>
        Adream
    </li>
    <li class="trm-post-copyright-link">
        <strong>本文链接：</strong>
        <a id="original-link" href="https://www.adream.icu/2021/06/12/CommonLossFunction/" title="十二大常见的损失函数">https://www.adream.icu/2021/06/12/CommonLossFunction/</a>
    </li>
    <li class="trm-post-copyright-license">
        <strong>版权声明：</strong>
        本博客所有文章除特别声明外，均默认采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" target="_blank" rel="noopener" title="CC BY-NC-SA 4.0 ">CC BY-NC-SA 4.0</a> 许可协议。
    </li>
</ul>


</div>
<div class="trm-post-next-prev row">
    <div class="col-lg-12">
        <!-- title -->
        <h5 class="trm-title-with-divider">
            其他文章
            <span data-number="02"></span>
        </h5>
    </div>
    
        <div class="col-lg-6">
    <div class="trm-older-publications-card trm-scroll-animation trm-active-el">
        <div class="trm-older-publication">
            
            <a class="trm-op-top trm-anima-link" href="/2021/06/12/DigitalImageProcessing-WaveletTransform/">
                <span class="trm-op-cover">
                    <img alt="cover" class="no-fancybox" src="/img/post/DigitalImageProcessing-WaveletTransform/picture1.png">
                </span>
                <h6 class="trm-op-title">小波变换</h6>
            </a>
            <div class="trm-divider trm-mb-15 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>21/06/12</li>
                <li>23:20</li>
                <li>学习记录类</li>
            </ul>
        </div>
    </div>
</div>
    
    
        <div class="col-lg-6">
    <div class="trm-older-publications-card trm-scroll-animation trm-active-el">
        <div class="trm-older-publication">
            
            <a class="trm-op-top trm-anima-link" href="/2021/06/11/LogisticRegression/">
                <span class="trm-op-cover">
                    <img alt="cover" class="no-fancybox" src="/img/post/LogisticRegression/1-S%E5%9E%8B%E6%9B%B2%E7%BA%BF.png">
                </span>
                <h6 class="trm-op-title">逻辑回归二分类</h6>
            </a>
            <div class="trm-divider trm-mb-15 trm-mt-20"></div>
            <ul class="trm-card-data trm-label">
                <li>21/06/11</li>
                <li>12:20</li>
                <li>学习记录类</li>
            </ul>
        </div>
    </div>
</div>
    
</div>

    



                    <div class="trm-divider footer-divider"></div>

                    <!-- footer -->
                    <footer class="trm-scroll-animation">

    

    
        <div class="trm-footer-item">
            <span>© 2021- 2024</span>
            <span class="footer-separator"data-separator=" · "></span>
            <span class="trm-accent-color">Adream</span>
        </div>
    

    
        <div class="trm-footer-item">
            <span>
                由 <a href="https://hexo.io" target="_blank" rel="noopener">Hexo</a> 驱动 v7.1.1
            </span>
            <span class="footer-separator" data-separator=" | "></span>
            <span> 
                主题 - 
                <a rel="noopener" href='https://github.com/MaLuns/hexo-theme-async' target='_blank'>Async</a>
                v2.1.10
            </span>
        </div>
      

    
        <div class="trm-footer-item blog-run-long"></div>
     

     
</footer>
                    <!-- footer end -->

                </div>
            </div>
        </div>
    </div>
</div>
            <!-- body end -->

            

    <div id="post-toc" class="trm-post-toc">
      <div class="trm-post-toc-header">
        目录导航
				<span id="post-toc-top">
					置顶
				</span>
      </div>
      <div class="trm-post-toc-content">
        <ol class="trm-toc"><li class="trm-toc-item trm-toc-level-1" title="损失函数"><a rel="nofollow" class="trm-toc-link" href="#损失函数"><span class="trm-toc-text">损失函数</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-2" title="1. 均方误差（MSE）"><a rel="nofollow" class="trm-toc-link" href="#1-均方误差（MSE）"><span class="trm-toc-text">1. 均方误差（MSE）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="1.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#1-1-介绍"><span class="trm-toc-text">1.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="1.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#1-2-数学公式"><span class="trm-toc-text">1.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="1.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#1-3-工作原理"><span class="trm-toc-text">1.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="1.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#1-4-纯-Python-代码实现"><span class="trm-toc-text">1.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="1.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#1-5-优缺点"><span class="trm-toc-text">1.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="2. 平均绝对误差（MAE）"><a rel="nofollow" class="trm-toc-link" href="#2-平均绝对误差（MAE）"><span class="trm-toc-text">2. 平均绝对误差（MAE）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="2.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#2-1-介绍"><span class="trm-toc-text">2.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="2.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#2-2-数学公式"><span class="trm-toc-text">2.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="2.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#2-3-工作原理"><span class="trm-toc-text">2.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="2.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#2-4-纯-Python-代码实现"><span class="trm-toc-text">2.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="2.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#2-5-优缺点"><span class="trm-toc-text">2.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="3. hinge 损失函数（Hinge Loss）"><a rel="nofollow" class="trm-toc-link" href="#3-hinge-损失函数（Hinge-Loss）"><span class="trm-toc-text">3. hinge 损失函数（Hinge Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="3.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#3-1-介绍"><span class="trm-toc-text">3.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="3.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#3-2-数学公式"><span class="trm-toc-text">3.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="3.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#3-3-工作原理"><span class="trm-toc-text">3.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="3.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#3-4-纯-Python-代码实现"><span class="trm-toc-text">3.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="3.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#3-5-优缺点"><span class="trm-toc-text">3.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="4. 指数损失函数（Exponential Loss）"><a rel="nofollow" class="trm-toc-link" href="#4-指数损失函数（Exponential-Loss）"><span class="trm-toc-text">4. 指数损失函数（Exponential Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="4.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#4-1-介绍"><span class="trm-toc-text">4.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="4.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#4-2-数学公式"><span class="trm-toc-text">4.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="4.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#4-3-工作原理"><span class="trm-toc-text">4.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="4.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#4-4-纯-Python-代码实现"><span class="trm-toc-text">4.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="4.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#4-5-优缺点"><span class="trm-toc-text">4.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="5. huber 损失函数（Huber Loss）"><a rel="nofollow" class="trm-toc-link" href="#5-huber-损失函数（Huber-Loss）"><span class="trm-toc-text">5. huber 损失函数（Huber Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="5.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#5-1-介绍"><span class="trm-toc-text">5.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="5.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#5-2-数学公式"><span class="trm-toc-text">5.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="5.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#5-3-工作原理"><span class="trm-toc-text">5.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="5.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#5-4-纯-Python-代码实现"><span class="trm-toc-text">5.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="5.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#5-5-优缺点"><span class="trm-toc-text">5.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="6. KL 散度函数（相对熵）"><a rel="nofollow" class="trm-toc-link" href="#6-KL-散度函数（相对熵）"><span class="trm-toc-text">6. KL 散度函数（相对熵）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="6.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#6-1-介绍"><span class="trm-toc-text">6.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="6.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#6-2-数学公式"><span class="trm-toc-text">6.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="6.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#6-3-工作原理"><span class="trm-toc-text">6.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="6.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#6-4-纯-Python-代码实现"><span class="trm-toc-text">6.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="6.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#6-5-优缺点"><span class="trm-toc-text">6.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="7. 交叉熵损失（Cross-Entropy Loss）"><a rel="nofollow" class="trm-toc-link" href="#7-交叉熵损失（Cross-Entropy-Loss）"><span class="trm-toc-text">7. 交叉熵损失（Cross-Entropy Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="7.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#7-1-介绍"><span class="trm-toc-text">7.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="7.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#7-2-数学公式"><span class="trm-toc-text">7.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="7.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#7-3-工作原理"><span class="trm-toc-text">7.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="7.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#7-4-纯-Python-代码实现"><span class="trm-toc-text">7.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="7.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#7-5-优缺点"><span class="trm-toc-text">7.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="8. 逻辑回归损失函数（Logistic Loss）"><a rel="nofollow" class="trm-toc-link" href="#8-逻辑回归损失函数（Logistic-Loss）"><span class="trm-toc-text">8. 逻辑回归损失函数（Logistic Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="8.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#8-1-介绍"><span class="trm-toc-text">8.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="8.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#8-2-数学公式"><span class="trm-toc-text">8.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="8.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#8-3-工作原理"><span class="trm-toc-text">8.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="8.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#8-4-纯-Python-代码实现"><span class="trm-toc-text">8.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="8.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#8-5-优缺点"><span class="trm-toc-text">8.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="9. 对数双曲余弦损失（Log-Cosh Loss）"><a rel="nofollow" class="trm-toc-link" href="#9-对数双曲余弦损失（Log-Cosh-Loss）"><span class="trm-toc-text">9. 对数双曲余弦损失（Log-Cosh Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="9.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#9-1-介绍"><span class="trm-toc-text">9.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="9.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#9-2-数学公式"><span class="trm-toc-text">9.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="9.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#9-3-工作原理"><span class="trm-toc-text">9.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="9.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#9-4-纯-Python-代码实现"><span class="trm-toc-text">9.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="9.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#9-5-优缺点"><span class="trm-toc-text">9.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="10. 对数双曲正切损失（Log-Hyperbolic Tangent Loss）"><a rel="nofollow" class="trm-toc-link" href="#10-对数双曲正切损失（Log-Hyperbolic-Tangent-Loss）"><span class="trm-toc-text">10. 对数双曲正切损失（Log-Hyperbolic Tangent Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="10.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#10-1-介绍"><span class="trm-toc-text">10.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="10.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#10-2-数学公式"><span class="trm-toc-text">10.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="10.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#10-3-工作原理"><span class="trm-toc-text">10.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="10.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#10-4-纯-Python-代码实现"><span class="trm-toc-text">10.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="10.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#10-5-优缺点"><span class="trm-toc-text">10.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="11. 余弦相似度损失（Cosine Similarity Loss）"><a rel="nofollow" class="trm-toc-link" href="#11-余弦相似度损失（Cosine-Similarity-Loss）"><span class="trm-toc-text">11. 余弦相似度损失（Cosine Similarity Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="11.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#11-1-介绍"><span class="trm-toc-text">11.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="11.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#11-2-数学公式"><span class="trm-toc-text">11.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="11.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#11-3-工作原理"><span class="trm-toc-text">11.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="11.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#11-4-纯-Python-代码实现"><span class="trm-toc-text">11.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="11.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#11-5-优缺点"><span class="trm-toc-text">11.5 优缺点</span></a></li></ol></li><li class="trm-toc-item trm-toc-level-2" title="12. 感知损失（Perceptual Loss）"><a rel="nofollow" class="trm-toc-link" href="#12-感知损失（Perceptual-Loss）"><span class="trm-toc-text">12. 感知损失（Perceptual Loss）</span></a><ol class="trm-toc-child"><li class="trm-toc-item trm-toc-level-3" title="12.1 介绍"><a rel="nofollow" class="trm-toc-link" href="#12-1-介绍"><span class="trm-toc-text">12.1 介绍</span></a></li><li class="trm-toc-item trm-toc-level-3" title="12.2 数学公式"><a rel="nofollow" class="trm-toc-link" href="#12-2-数学公式"><span class="trm-toc-text">12.2 数学公式</span></a></li><li class="trm-toc-item trm-toc-level-3" title="12.3 工作原理"><a rel="nofollow" class="trm-toc-link" href="#12-3-工作原理"><span class="trm-toc-text">12.3 工作原理</span></a></li><li class="trm-toc-item trm-toc-level-3" title="12.4 纯 Python 代码实现"><a rel="nofollow" class="trm-toc-link" href="#12-4-纯-Python-代码实现"><span class="trm-toc-text">12.4 纯 Python 代码实现</span></a></li><li class="trm-toc-item trm-toc-level-3" title="12.5 优缺点"><a rel="nofollow" class="trm-toc-link" href="#12-5-优缺点"><span class="trm-toc-text">12.5 优缺点</span></a></li></ol></li></ol></li></ol>
      </div>
    </div>

            <div class="trm-fixed-container"><div class="trm-fixed-btn post-toc-btn" data-title="目录"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-liebiao"></use>
</svg></div><div class="trm-fixed-btn" id="trm-search-btn" data-title="查询"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-chaxun"></use>
</svg></div><div class="trm-fixed-btn" data-title="切换主题模式" onclick="asyncFun.switchThemeMode(document.documentElement.classList.contains('dark')?'style-light':'style-dark')"><i class="fas fa-sun"></i></div><div class="trm-fixed-btn" data-title="阅读模式" onclick="asyncFun.switchReadMode()"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-yuedu"></use>
</svg></div><div class="trm-fixed-btn hidden-md" data-title="单栏和双栏切换" onclick="asyncFun.switchSingleColumn()"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-arrows-h"></use>
</svg></div><div class="trm-fixed-btn" id="trm-back-top" data-title="回到顶部"><svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-backtop"></use>
</svg></div></div>
        </div>
      </div>
      <!-- scroll container end -->
  </div>
  <!-- app wrapper end -->

  
    <div class="trm-search-popup">
        <div class="trm-search-wrapper">
            <div class="form trm-search-form">
                <div class="trm-search-input-icon">
                    <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-chaxun"></use>
</svg>
                </div>
                <input class="trm-search-input" type="text" placeholder="搜索文章...">
                <div class="trm-search-btn-close">
                    <svg class="symbol-icon " aria-hidden="true">
    <use xlink:href="#icon-guanbi"></use>
</svg>
                </div>
            </div>
            <div class="trm-search-result-container">
                <div class="trm-search-empty">
                    请输入关键词进行搜索
                </div>
            </div>
            <div class="trm-search-footer">
                <div class="trm-search-stats"></div>
                <ul class="trm-search-commands">
                    <li>
                        <kbd class="command-palette-commands-key">
                            <svg width="15" height="15" aria-label="Escape key" role="img">
                                <g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"
                                    stroke-width="1.2">
                                    <path
                                        d="M13.6167 8.936c-.1065.3583-.6883.962-1.4875.962-.7993 0-1.653-.9165-1.653-2.1258v-.5678c0-1.2548.7896-2.1016 1.653-2.1016.8634 0 1.3601.4778 1.4875 1.0724M9 6c-.1352-.4735-.7506-.9219-1.46-.8972-.7092.0246-1.344.57-1.344 1.2166s.4198.8812 1.3445.9805C8.465 7.3992 8.968 7.9337 9 8.5c.032.5663-.454 1.398-1.4595 1.398C6.6593 9.898 6 9 5.963 8.4851m-1.4748.5368c-.2635.5941-.8099.876-1.5443.876s-1.7073-.6248-1.7073-2.204v-.4603c0-1.0416.721-2.131 1.7073-2.131.9864 0 1.6425 1.031 1.5443 2.2492h-2.956">
                                    </path>
                                </g>
                            </svg>
                        </kbd>
                        <span class="command-palette-Label">to close</span>
                    </li>
                </ul>
            </div>
        </div>
    </div>

  <!-- Plugin -->




    
<script src="https://npm.elemecdn.com/swup@2.0.19/dist/swup.min.js"></script>

    
<script src="https://npm.elemecdn.com/@fancyapps/ui@4.0/dist/fancybox.umd.js"></script>

    

    
        <script src="/js/plugins/typing.js?v=2.1.10"></script>
    

    
        
<script src="https://npm.elemecdn.com/hexo-generator-searchdb@1.4.0/dist/search.js"></script>

        <script src="/js/plugins/local_search.js?v=2.1.10"></script>
    

    <!-- 数学公式 -->
    
        
<script src="https://npm.elemecdn.com/katex@latest/dist/katex.min.js" data-swup-reload-script></script>

        
            
<script src="https://npm.elemecdn.com/katex@latest/dist/contrib/copy-tex.min.js" data-swup-reload-script></script>

        
        
<script src="https://npm.elemecdn.com/katex@latest/dist/contrib/auto-render.min.js" data-swup-reload-script></script>

        <script data-swup-reload-script>
              window.renderMathInElement(document.body, {
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                ],
                ...{},
            })
        </script>
    

    <!-- 评论插件 -->
    
        

        
    



<!-- CDN -->


    

    

    




    <!-- Service Worker -->
    
    <!-- baidu push -->
    


<script id="async-script" src="/js/main.js?v=2.1.10"></script>

</body>

</html>